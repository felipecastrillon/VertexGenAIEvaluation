# VertexGenAIEvaluation

This package provides some utilities to make it easy to perform evaluation tasks with Google Cloud VertexAI models. Ideally you will only need a few lines of code to perform this task.

### Requirements 

The following requirements are necessary to run this project:
- A Google Cloud Platform (GCP) project
- IAM user access to VertexAI
- Authentication to GCP, (see below section to auth via service account)
- A "golden dataset" containing a row with the prompt context and another row with "ground truth" answers
- If evaluating a tuned model, [the tuned model should already be created and deployed] (https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models). 

### How to Run

First, we need to authenticate via the terminal. One way to authenticate to GCP is to [create a service account] (https://cloud.google.com/iam/docs/keys-create-delete) and download it. Then run the terminal command:

```bash
export GOOGLE_APPLICATION_CREDENTIALS=<file_location>
```

Now that we are authenticated open up a Python environment and get started. First let's create our model object; there are two types of models that can be created: a vanilla Palm Text model (not tuned) and a tuned Palm Text model. For the latter you will have to [tune the model on the console or via the API/SDK] (https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models). Here is an example of a model:



```python
model_instance =  models.PalmBisonModel(parameters = {"temperature":0.0,
                                                "top_k":5,
                                                "top_p":0.8,
                                                "max_output_tokens":1000}  )
# or...
tuned_model_instance = models.PalmBisonModel(parameters = {"temperature":0.0,
                                                "top_k":5,
                                                "top_p":0.8,
                                                "max_output_tokens":1000},
                                             endpoint_path=<MODEL_ENDPOINT>  )
```



Now let's read the data and populate some generated answers for each row in the dataset. The prompt will be generated by combining the prefix question and the values from the context_col. 

```python
prefix = "summarize the following article: "
data_instance = data.Data(data_loc="path/to/summarization_sample.jsonl", prefix_question=prefix, context_col="article", ground_truth_col="summary", llm_model=model_instance)

```

Finally let's do an evaluation task comparing the generated response vs the "ground truth" response and get the mean score:

```python
evaluator_instance = evaluators.SemanticSimilarityEvaluator(data=data_instance)
mean_score = evaluator_instance.evaluation_job()

```

### Evaluation Metrics

For the last task, I have incorporated 4 evaluator classes to compare the "generated response" vs the "ground truth":

- **SemanticSimilarityEvaluator()** - This metric is useful when determining how similar the two texts are on a semantic basis. This will give a score between 0 and 1. This is a great metric for  evaluating summarization or content retrieval tasks.
- **BleuEvaluator()** - The [BLEU metric] (https://en.wikipedia.org/wiki/BLEU) can help evaluate the token similarity between two texts. In this implementation we are only using the 1-gram BLEU evaluator. Could be used for summarization of content retrieval tasks. 
- **ExactMatchEvaluator()** - This metric evalutes an exact match between the two texts. This is useful for classification tasks or tasks expecting exact answers. 
- **SentimentEvaluator()** - Evaluate sentiment tasks Yes/No tasks. 

### Ideas on Extensions

- Integrate with Langchain
- Add Enterprise Search as a model
- Add 3rd party models
- Add more Evalutors
- Import and export data from BQ/GCS
- Run evaluators on Dataflow for better scale
